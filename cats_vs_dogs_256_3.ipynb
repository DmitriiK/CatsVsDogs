{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'dima'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "\n",
    "import Image\n",
    "import ImageOps\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def jpg_to_nparray(path, img_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    counter = 0\n",
    "\n",
    "    for img_dir in os.listdir(path):\n",
    "\n",
    "        # X\n",
    "        img = Image.open(path+img_dir)\n",
    "\n",
    "        img = ImageOps.fit(img, (img_size, img_size), Image.ANTIALIAS)\n",
    "        #img = ImageOps.grayscale(img)\n",
    "\n",
    "        # If want to plot some of the images\n",
    "        #if counter%3000 == 0:\n",
    "            #imgplot = plt.imshow(img)\n",
    "\n",
    "\n",
    "        img = np.asarray(img, dtype = 'float32') / 255.\n",
    "        img = img.reshape(3, img_size, img_size)\n",
    "        X.append(img)\n",
    "\n",
    "        # Y: 0 for cat, 1 for dog\n",
    "        if \"cat\" in img_dir:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "\n",
    "\n",
    "        # Printing\n",
    "        counter+=1\n",
    "        if counter%6000 == 0:\n",
    "            print'processed images: ', counter\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y,dtype='int32')\n",
    "\n",
    "    return (X,Y)\n",
    "\n",
    "\n",
    "def get_ids(path):\n",
    "    ids = np.array([],dtype = int)\n",
    "    for str in os.listdir(path):\n",
    "        ids = np.append(ids, int(str.partition(\".\")[0]))\n",
    "    \n",
    "    ids = np.array(ids, dtype = int)[...,None]\n",
    "    return ids\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def make_datasets(img_size):\n",
    "\n",
    "    train_path = '/media/dima/Data/DATA_DL/cats_vs_dogs/train/'\n",
    "    unlabeled_path = '/media/dima/Data/DATA_DL/cats_vs_dogs/test1/'\n",
    "\n",
    "    print 'Prosessing labeled images'\n",
    "\n",
    "    X_train, Y_train = jpg_to_nparray(train_path, img_size)\n",
    "\n",
    "    # Patrtition into labeled training, validatin and test sets\n",
    "    # Shuffle the examples, because they are in order in the original dataset with all cats being first\n",
    "    ind = np.arange(25000)\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(ind)\n",
    "\n",
    "    X_train = X_train[ind]\n",
    "    Y_train = Y_train[ind]\n",
    "\n",
    "    X_val = X_train[0:2999]\n",
    "    Y_val = Y_train[0:2999]\n",
    "    X_test = X_train[3000:4999]\n",
    "    Y_test = Y_train[3000:4999]\n",
    "    X_train = X_train[5000:24999]\n",
    "    Y_train = Y_train[5000:24999]\n",
    "\n",
    "    print 'Prosessing unlabeled images'\n",
    "\n",
    "    X_unlabeled, Y_unlabeled = jpg_to_nparray(unlabeled_path, img_size)\n",
    "\n",
    "    pickle.dump((X_train, Y_train, X_val, Y_val, X_test, Y_test, X_unlabeled, Y_unlabeled), \n",
    "                open('/media/dima/Data/DATA_DL/cats_vs_dogs/datasets256.pkl','wb'))\n",
    "\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 100, 100),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(7, 7),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=64, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    #########\n",
    "#####################\n",
    "###############\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=64, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    ############\n",
    "    ############\n",
    "###############\n",
    "\n",
    "\n",
    "    ##########\n",
    "    # One more\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=100, filter_size=(4, 4),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=400,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(num_epochs=30):\n",
    "\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, X_unlabeled, y_unlabeled = pickle.load(\n",
    "        open(os.path.abspath('/media/dima/Data/DATA_DL/cats_vs_dogs/datasets.pkl'),'rb'))\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "\n",
    "    network = build_cnn(input_var)\n",
    "\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    l2_regularization = lasagne.regularization.regularize_network_params(network, lasagne.regularization.l2, tags={'regularizable': True})*0.0006\n",
    "    loss = loss + l2_regularization\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.002, momentum=0.8)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_loss = test_loss + l2_regularization\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates,  allow_input_downcast=True)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    get_prediction = theano.function([input_var], [test_prediction])\n",
    "\n",
    "    \n",
    "    with np.load('100_grayscale_convnet_NEW_cat_dog_model.npz') as f:\n",
    "         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "    #Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 250, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "    \n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 250, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "    \n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "        \n",
    "        # Save weights every 5 epochs\n",
    "        if (epoch%5 == 0):\n",
    "            np.savez('100_grayscale_convnet_NEW_cat_dog_model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "\n",
    "\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 250, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    predictions = np.array([],dtype = int)\n",
    "    # Make predictions\n",
    "    for batch in iterate_minibatches(X_unlabeled, y_unlabeled, 250, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        batch_predictions = np.array(get_prediction(inputs)).reshape(250,2)\n",
    "        batch_predictions = batch_predictions[:,1]>batch_predictions[:,0] \n",
    "        batch_predictions = np.array(batch_predictions, dtype= int)\n",
    "        predictions = np.append(predictions, batch_predictions)\n",
    "    predictions = np.array(predictions, dtype = int)[...,None]\n",
    "    \n",
    "    unlabeled_path = os.path.abspath('/media/dima/Data/DATA_DL/cats_vs_dogs/test1/')\n",
    "    submission_ids = get_ids(unlabeled_path)\n",
    "    \n",
    "    submission_array = (np.append(submission_ids, predictions,axis = 1))\n",
    "    np.savetxt(\"submission.csv\", submission_array,  fmt='%d', delimiter=',', \n",
    "               newline='\\n', header='id,label', comments = '')\n",
    "    print(\"Submission file saved\")\n",
    "\n",
    "\n",
    "\n",
    "    # Save the network weights to a file:\n",
    "    np.savez('100_grayscale_convnet_NEW_cat_dog_model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    # And load them again later on like this:\n",
    "    with np.load('100_grayscale_convnet_NEW_cat_dog_model.npz') as f:\n",
    "         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "\n",
    "def visualize():\n",
    "\n",
    "    #Load network params\n",
    "    with np.load('100_grayscale_convnet_87_cat_dog_model.npz') as f:\n",
    "         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    make_datasets(256)\n",
    "    #train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
